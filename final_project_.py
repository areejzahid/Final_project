# -*- coding: utf-8 -*-
"""Final_Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dOsXtbvmRg4wYrVzXQ_oxM9aNR_K4SxJ
"""

# STEP 1: Load and Extract ZIP (Colab compatible)

import zipfile
import io
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from google.colab import files  # Remove this line if using local Jupyter

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# üìÅ Upload ZIP file
uploaded = files.upload()  # This will prompt to upload 'news.zip'

# Extract uploaded ZIP
zip_file_name = list(uploaded.keys())[0]  # Get uploaded zip file name
with zipfile.ZipFile(io.BytesIO(uploaded[zip_file_name]), 'r') as z:
    z.extractall('/tmp/')  # Extract to temp directory

# üìä Load CSVs
df_fake = pd.read_csv('/tmp/Fake.csv')
df_real = pd.read_csv('/tmp/True.csv')

# Show heads
display(df_fake.head())
display(df_real.head())

# üè∑Ô∏è Add labels
df_fake['label'] = 0
df_real['label'] = 1

# üìå Combine and shuffle
df = pd.concat([df_fake, df_real], ignore_index=True)
df = df.sample(frac=1).reset_index(drop=True)

print("Columns of loaded data after labeling:", df.columns)
display(df.head())

# üîç Remove missing text rows
df.dropna(subset=['text'], inplace=True)
print("Columns after removing missing text:", df.columns)
display(df.head())

#STEP 2. Text Preprocessing / Cleaning
# Lowercase + Remove unwanted characters
import re
import string # Import the string module
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download required NLTK resources if not already downloaded in Step 1
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')


# Initialize stopwords and lemmatizer once
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = str(text).lower() # Ensure text is string and convert to lowercase
    text = re.sub(r"http\S+|www\S+", "", text)  # Remove URLs
    text = re.sub(r"[^a-zA-Z\s]", "", text)     # Remove punctuation/numbers
    text = ' '.join([word for word in text.split() if word not in stop_words]) # Remove stopwords
    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()]) # Lemmatize
    return text

df['clean_text'] = df['text'].apply(clean_text)
print("Columns after text cleaning:", df.columns)
display(df.head())

# Tokenization is not needed as a separate column for this pipeline

# Final clean view
display(df[['clean_text', 'label']].head())
print("Final columns:", df.columns)

#STEP 3. Exploratory Data Analysis (EDA)

#Fake vs Real Distribution (Count Plot)
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.countplot(data=df, x='label', palette='viridis')
plt.xticks([0, 1], ['Fake', 'Real'])
plt.title("Distribution of Fake vs Real News")
plt.xlabel("News Type")
plt.ylabel("Count")
plt.show()

# Article Length Distribution by Class
# Add text length column
df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))

# Plot
plt.figure(figsize=(10,6))
sns.histplot(data=df, x='text_length', hue='label', bins=50, kde=True, palette='coolwarm')
plt.title("Article Word Count Distribution")
plt.xlabel("Number of Words in News")
plt.ylabel("Frequency")
plt.legend(labels=["Fake", "Real"])
plt.show()

#WordCloud for Fake News

from wordcloud import WordCloud

# Combine all fake news text
fake_text = " ".join(df[df['label'] == 0]['text'].dropna())

# Generate wordcloud
wordcloud_fake = WordCloud(width=800, height=400, background_color='black').generate(fake_text)

# Plot
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_fake, interpolation='bilinear')
plt.axis('off')
plt.title("Most Frequent Words in Fake News")
plt.show()

# WordCloud for Real News
# Combine all real news text
real_text = " ".join(df[df['label'] == 1]['text'].dropna())

# Generate wordcloud
wordcloud_real = WordCloud(width=800, height=400, background_color='white').generate(real_text)

# Plot
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_real, interpolation='bilinear')
plt.axis('off')
plt.title("Most Frequent Words in Real News")
plt.show()

#STEP 4. Model Building (Naive Bayes)
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Vectorization
tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(df['clean_text'])
y = df['label']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = MultinomialNB()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

print(X_train.shape)  # (~35900, 5000)
print(X_test.shape)   # (~9000, 5000)

#STEP 5. Evaluation Metrics

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title("Confusion Matrix")
plt.show()

#Fake News Delete Karna (from the DataFrame)

df_cleaned = df[df['label'] == 1].reset_index(drop=True)
display(df_cleaned.head())
print(f"Original dataset size: {len(df)}")
print(f"After removing fake news: {len(df_cleaned)}")